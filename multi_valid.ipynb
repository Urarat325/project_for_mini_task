{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import decimal\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def read_file(path, encoding, is_lower=False):\n",
    "    list_of_paragraph = []\n",
    "    buffer = []\n",
    "    with open(path, encoding=encoding) as file:\n",
    "        regex = re.compile(\n",
    "            r'(((?<!Статья )(?<!^)(?<!(\\.|\\s))(?<!     )[а-яА-Я\\d]{2,})([\\.]{0,1}[\\]\\)\\\"]{0,2}[\\.\\;\\:]{1}(\\s|$)))',\n",
    "            flags=re.IGNORECASE)\n",
    "        text = file.read()\n",
    "\n",
    "        if is_lower:\n",
    "            text = text.lower()\n",
    "        offset = 0\n",
    "        buf = ''\n",
    "        for ind, value in enumerate(regex.split(text, maxsplit=0)):\n",
    "            if ind == 0 + offset:\n",
    "                buf = value\n",
    "            if ind == 1 + offset:\n",
    "                list_of_paragraph.append((buf + value).strip())\n",
    "                offset += 6\n",
    "\n",
    "        second_regex = re.compile(r'([^\\dгст][\\.\\;\\:](?=\\s|$))', flags=re.IGNORECASE)\n",
    "\n",
    "        for value1 in list_of_paragraph:\n",
    "            buf = ''\n",
    "            for value in second_regex.split(value1, maxsplit=0):\n",
    "                if value == '':\n",
    "                    continue\n",
    "\n",
    "                if len(value) > 2 and buf != '':\n",
    "                    buffer.append(buf.strip())\n",
    "                    buf = value\n",
    "                elif len(value) > 2:\n",
    "                    buf = value\n",
    "                elif len(value) <= 2:\n",
    "                    buffer.append((buf + value).strip())\n",
    "                    buf = ''\n",
    "    return buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting file paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "list_of_path = list(filter(lambda x: not '1ДИ' in str(x), glob.glob(\"docs/*/*.txt\")))\n",
    "list_of_path\n",
    "\n",
    "list_of_instruction_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список ДИ'), list_of_path))\n",
    "list_of_external_doc_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список внешних'), list_of_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation of sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "list_of_external_docs = []\n",
    "list_of_instructions = []\n",
    "\n",
    "in_lower_case = True\n",
    "\n",
    "for path_to_doc in list_of_instruction_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [x for x in buf_list if len(x) > 3]\n",
    "    list_of_instructions.append(buf_list)\n",
    "\n",
    "for path_to_doc in list_of_external_doc_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [x for x in buf_list if len(x) > 3]\n",
    "    list_of_external_docs.append(buf_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "download_first_model = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "embed = None\n",
    "size = 512\n",
    "if download_first_model:\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "else:\n",
    "    size = 768\n",
    "    embed = SentenceTransformer('nq-distilbert-base-v1', device='cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from instructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 52.4 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_instruction_embeddings = []\n",
    "\n",
    "for doc in list_of_instructions:\n",
    "    buffer = np.empty((0, size), float)\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "\n",
    "    list_of_instruction_embeddings.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from external docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 7s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_embeddings_of_external_docs = []\n",
    "\n",
    "for doc in list_of_external_docs:\n",
    "    buffer = np.empty((0, size), float)\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "\n",
    "    list_of_embeddings_of_external_docs.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing similarity matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "list_for_df = []\n",
    "\n",
    "for index, doc1 in enumerate(list_of_instruction_embeddings):\n",
    "    buffer = {\n",
    "        'instruction_filename': list_of_instruction_paths[index]\n",
    "    }\n",
    "    for ind, doc2 in enumerate(list_of_embeddings_of_external_docs):\n",
    "        buffer['external_doc_filename'] = list_of_external_doc_paths[ind]\n",
    "        if download_first_model:\n",
    "            similarity_matrix = np.inner(doc1, doc2)\n",
    "        else:\n",
    "            similarity_matrix = cosine_similarity(doc1, doc2)\n",
    "\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            buffer['sentence_number_from_instruction'] = i\n",
    "            buffer['sentence_number_from_external_doc'] = int(np.argmax(row))\n",
    "            buffer['value'] = np.max(row)\n",
    "            buffer['text_of_instruction'] = list_of_instructions[index][i]\n",
    "            buffer['text_of_external_doc'] = list_of_external_docs[ind][int(np.argmax(row))]\n",
    "\n",
    "            list_for_df.append(buffer.copy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'sentence_number_from_instruction',\n",
    "        'sentence_number_from_external_doc',\n",
    "        'value',\n",
    "        'text_of_instruction',\n",
    "        'instruction_filename',\n",
    "        'text_of_external_doc',\n",
    "        'external_doc_filename'\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "for index, row in enumerate(list_for_df):\n",
    "    new_df.loc[index] = [\n",
    "        row['sentence_number_from_instruction'],\n",
    "        row['sentence_number_from_external_doc'],\n",
    "        row['value'],\n",
    "        row['text_of_instruction'],\n",
    "        row['instruction_filename'],\n",
    "        row['text_of_external_doc'],\n",
    "        row['external_doc_filename'],\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Файл создан\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(\"embeddings.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    new_df.to_excel(writer, 'good', engine='xlsxwriter')\n",
    "    sheets_good = writer.sheets['good']\n",
    "    sheets_good.autofilter(0, 0, new_df.shape[0], new_df.shape[1])\n",
    "\n",
    "    print(\"Файл создан\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
