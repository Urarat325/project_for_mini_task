{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import decimal\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def read_file(path: str, encoding: str, is_lower=False):\n",
    "    list_of_paragraph = []\n",
    "    buffer = []\n",
    "    with open(path, encoding=encoding) as file:\n",
    "        regex = re.compile(\n",
    "            r'(((?<!Статья )(?<!^)(?<!(\\.|\\s))(?<!     )[а-яА-Я\\d]{2,})([\\.]{0,1}[\\]\\)\\\"]{0,2}[\\.\\;\\:]{1}(\\s|$)))',\n",
    "            flags=re.IGNORECASE)\n",
    "        text = file.read()\n",
    "\n",
    "        if is_lower:\n",
    "            text = text.lower()\n",
    "        offset = 0\n",
    "        buf = ''\n",
    "        for ind, value in enumerate(regex.split(text, maxsplit=0)):\n",
    "            if ind == 0 + offset:\n",
    "                buf = value\n",
    "            if ind == 1 + offset:\n",
    "                list_of_paragraph.append((buf + value).strip())\n",
    "                offset += 6\n",
    "\n",
    "        second_regex = re.compile(r'([^\\dгст][\\.\\;\\:](?=\\s|$))', flags=re.IGNORECASE)\n",
    "\n",
    "        for value1 in list_of_paragraph:\n",
    "            buf = ''\n",
    "            for value in second_regex.split(value1, maxsplit=0):\n",
    "                if value == '':\n",
    "                    continue\n",
    "\n",
    "                if len(value) > 2 and buf != '':\n",
    "                    buffer.append(buf.strip())\n",
    "                    buf = value\n",
    "                elif len(value) > 2:\n",
    "                    buf = value\n",
    "                elif len(value) <= 2:\n",
    "                    buffer.append((buf + value).strip())\n",
    "                    buf = ''\n",
    "    return buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting file paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "list_of_path = list(filter(lambda x: not '1ДИ' in str(x), glob.glob(\"docs/*/*.txt\")))\n",
    "\n",
    "list_of_instruction_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список ДИ'), list_of_path))\n",
    "list_of_external_doc_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список внешних'), list_of_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation of sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def remove_all_specific_symbols(text: str):\n",
    "    text = re.sub(r'[\\.\\,\\;\\:]', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    bad_symbols = [')', '*']\n",
    "    for symbol in bad_symbols:\n",
    "        if text.startswith(symbol):\n",
    "            text = text[1:]\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    text = re.sub(r'(\\d+)(?:\\.)?', '', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def valid(text: str) -> bool:\n",
    "    def mini_valid(x: str):\n",
    "        list_of_specific_symbols = ['.', ',', ':', ';']\n",
    "        for sym in list_of_specific_symbols:\n",
    "            if sym in x:\n",
    "                return len([y for y in re.split(r'\\s', ' '.join(x.split(sym, maxsplit=0))) if len(y) > 1]) > 2\n",
    "        return True\n",
    "\n",
    "    flag: bool = len(list(filter(lambda x: len(str(x)) > 0 and mini_valid(x), re.split(r'\\s', text, maxsplit=0)))) > 2\n",
    "    return len(text) > 3 and flag"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.91 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_of_external_docs = []\n",
    "list_of_instructions = []\n",
    "\n",
    "in_lower_case = True\n",
    "\n",
    "for path_to_doc in list_of_instruction_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [clean(x) for x in buf_list]\n",
    "    buf_list = [remove_all_specific_symbols(x) for x in buf_list if valid(x)]\n",
    "    list_of_instructions.append(buf_list)\n",
    "\n",
    "for path_to_doc in list_of_external_doc_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [clean(x) for x in buf_list]\n",
    "    buf_list = [remove_all_specific_symbols(x) for x in buf_list if valid(x)]\n",
    "    list_of_external_docs.append(buf_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "download_first_model = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "embed = None\n",
    "size = 512\n",
    "if download_first_model:\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "else:\n",
    "    size = 768\n",
    "    embed = SentenceTransformer('nq-distilbert-base-v1', device='cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "bad_words = []\n",
    "with open('./stop.txt', encoding='windows-1251') as file:\n",
    "    for text in file.readlines():\n",
    "        if text.endswith('\\n'):\n",
    "            bad_words.append(embed(remove_all_specific_symbols(text[:-1])))\n",
    "            # bad_words.append(embed(text[:-1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from instructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.3 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_instruction_embeddings = []\n",
    "\n",
    "for doc in list_of_instructions:\n",
    "    buffer = np.empty((0, size), float)\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "\n",
    "    list_of_instruction_embeddings.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from external docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_embeddings_of_external_docs = []\n",
    "\n",
    "for doc in list_of_external_docs:\n",
    "    buffer = np.empty((0, size), float)\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "\n",
    "    list_of_embeddings_of_external_docs.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing similarity matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "0.06345158104623635"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.inner(bad_words[2], list_of_embeddings_of_external_docs[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.2315267 ],\n       [0.36904797]], dtype=float32)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(embed(['примечания:', 'примечания']),\n",
    "         embed('(подпись)\t(ф и о ) (подпись)\t(ф и о )(подпись)\t(ф и о ) примечания '))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.9215764 ],\n       [0.75968325]], dtype=float32)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(embed(['федеральный закон от № -фз (ред', '- федеральный закон от n -фз']),\n",
    "         embed('в силу с ) федеральный закон рф от № -фз (ред '))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 42s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_for_df = []\n",
    "\n",
    "for index, doc1 in enumerate(list_of_instruction_embeddings):\n",
    "    buffer = {\n",
    "        'instruction_filename': list_of_instruction_paths[index]\n",
    "    }\n",
    "    for ind, doc2 in enumerate(list_of_embeddings_of_external_docs):\n",
    "        buffer['external_doc_filename'] = list_of_external_doc_paths[ind]\n",
    "        if download_first_model:\n",
    "            similarity_matrix = np.inner(doc1, doc2)\n",
    "        else:\n",
    "            similarity_matrix = cosine_similarity(doc1, doc2)\n",
    "\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            flag = False\n",
    "            # embed_text = embed(list_of_instructions[index][i])\n",
    "            # embed_text2 = embed(list_of_external_docs[ind][int(np.argmax(row))])\n",
    "            for word in bad_words:\n",
    "                if np.max(np.inner(word, doc1[i])) >= 0.95 or np.max(np.inner(word, doc2[int(np.argmax(row))])) >= 0.95:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            buffer['sentence_number_from_instruction'] = i\n",
    "            buffer['sentence_number_from_external_doc'] = int(np.argmax(row))\n",
    "            buffer['value'] = np.max(row)\n",
    "            buffer['text_of_instruction'] = list_of_instructions[index][i]\n",
    "            buffer['text_of_external_doc'] = list_of_external_docs[ind][int(np.argmax(row))]\n",
    "\n",
    "            list_for_df.append(buffer.copy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'sentence_number_from_instruction',\n",
    "        'sentence_number_from_external_doc',\n",
    "        'value',\n",
    "        'text_of_instruction',\n",
    "        'instruction_filename',\n",
    "        'text_of_external_doc',\n",
    "        'external_doc_filename'\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 31s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in enumerate(list_for_df):\n",
    "    # new_df.loc[index] = [\n",
    "    #     row['sentence_number_from_instruction'],\n",
    "    #     row['sentence_number_from_external_doc'],\n",
    "    #     row['value'],\n",
    "    #     row['text_of_instruction'],\n",
    "    #     row['instruction_filename'],\n",
    "    #     row['text_of_external_doc'],\n",
    "    #     row['external_doc_filename'],\n",
    "    # ]\n",
    "    new_df = new_df.append({\n",
    "        'sentence_number_from_instruction': row['sentence_number_from_instruction'],\n",
    "        'sentence_number_from_external_doc': row['sentence_number_from_external_doc'],\n",
    "        'value': row['value'],\n",
    "        'text_of_instruction': row['text_of_instruction'],\n",
    "        'instruction_filename': row['instruction_filename'],\n",
    "        'text_of_external_doc': row['text_of_external_doc'],\n",
    "        'external_doc_filename': row['external_doc_filename'],\n",
    "    }, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл создан\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(\"embeddings.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    new_df.to_excel(writer, 'good', engine='xlsxwriter')\n",
    "    sheets_good = writer.sheets['good']\n",
    "    sheets_good.autofilter(0, 0, new_df.shape[0], new_df.shape[1])\n",
    "\n",
    "    print(\"Файл создан\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
